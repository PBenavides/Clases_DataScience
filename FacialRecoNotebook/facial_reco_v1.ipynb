{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ricardo/Personal/data_science/data_science/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ricardo/Personal/data_science/data_science/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ricardo/Personal/data_science/data_science/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ricardo/Personal/data_science/data_science/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ricardo/Personal/data_science/data_science/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ricardo/Personal/data_science/data_science/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.1.0 19.17.0 2.1.4-tf 1.7.0\n",
      "images/5_images/train models/dlib_face_recognition_resnet_model_v1.dat\n"
     ]
    }
   ],
   "source": [
    "from skimage import io\n",
    "from scipy import misc\n",
    "import os\n",
    "import openface\n",
    "import sys\n",
    "import cv2\n",
    "import dlib\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import imutils\n",
    "import glob\n",
    "import facenet\n",
    "import sklearn \n",
    "path_5 = os.path.join(\"images\")\n",
    "path_1 = os.path.join(\"images\", \"face_1.jpeg\")\n",
    "path_2 = os.path.join(\"models\",\"family_1.jpg\")\n",
    "path_3 = os.path.join(\"models\",\"dlib_face_recognition_resnet_model_v1.dat\")\n",
    "path_4 = os.path.join(\"models\",\"shape_predictor_68_face_landmarks.dat\")\n",
    "path_6 = os.path.join(\"images\",\"5_images\",\"train\")\n",
    "path_7 = os.path.join(\"images\",\"5_images\",\"val\")\n",
    "path_8 = os.path.join(\"images\",\"5_images\",\"new_train\")\n",
    "\n",
    "print(cv2.__version__,dlib.__version__,keras.__version__,tf.__version__)\n",
    "print(path_6,path_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detection 1: Left: 46 Top: 81 Right: 201 Bottom: 236\n",
      "Detection 1: Left: 12 Top: 81 Right: 167 Bottom: 236\n",
      "Detection 1: Left: 26 Top: 46 Right: 116 Bottom: 136\n",
      "Detection 1: Left: 121 Top: 32 Right: 183 Bottom: 94\n",
      "Detection 1: Left: 53 Top: 96 Right: 182 Bottom: 225\n",
      "Detection 1: Left: 91 Top: 68 Right: 199 Bottom: 175\n",
      "Detection 1: Left: 91 Top: 68 Right: 199 Bottom: 175\n",
      "Detection 1: Left: 52 Top: 32 Right: 114 Bottom: 94\n",
      "Detection 1: Left: 79 Top: 116 Right: 187 Bottom: 223\n",
      "Detection 1: Left: 46 Top: 76 Right: 135 Bottom: 166\n",
      "Detection 1: Left: 101 Top: 38 Right: 152 Bottom: 90\n",
      "Detection 1: Left: 115 Top: 116 Right: 223 Bottom: 223\n",
      "Detection 1: Left: 142 Top: 46 Right: 204 Bottom: 108\n",
      "Detection 1: Left: 73 Top: 59 Right: 135 Bottom: 122\n",
      "Detection 1: Left: 46 Top: 98 Right: 201 Bottom: 253\n",
      "Detection 1: Left: 107 Top: 39 Right: 170 Bottom: 101\n",
      "Detection 1: Left: 52 Top: 39 Right: 114 Bottom: 101\n",
      "Detection 1: Left: 38 Top: 96 Right: 167 Bottom: 225\n",
      "Detection 1: Left: 56 Top: 98 Right: 242 Bottom: 284\n",
      "Detection 1: Left: 55 Top: 88 Right: 129 Bottom: 163\n",
      "Detection 1: Left: 121 Top: 53 Right: 183 Bottom: 115\n",
      "Detection 1: Left: 121 Top: 53 Right: 183 Bottom: 115\n",
      "Detection 1: Left: 91 Top: 56 Right: 199 Bottom: 163\n",
      "Detection 1: Left: 121 Top: 55 Right: 196 Bottom: 130\n",
      "Detection 1: Left: 80 Top: 25 Right: 142 Bottom: 87\n"
     ]
    }
   ],
   "source": [
    "predictor_path = path_4\n",
    "face_rec_model_path = path_3\n",
    "\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "sp = dlib.shape_predictor(predictor_path)\n",
    "facerec = dlib.face_recognition_model_v1(face_rec_model_path)\n",
    "win = dlib.image_window()\n",
    "\n",
    "final_array_embeddings=[]\n",
    "final_array_data=[]\n",
    "final_array_label=[]\n",
    "\n",
    "for ft1 in glob.glob(os.path.join(path_6, \"*\")):\n",
    "    temp_array_data=[]\n",
    "    temp_array_label=[]\n",
    "    temp_array_embeddings=[]\n",
    "    for ft2 in glob.glob(os.path.join(ft1,\"*.jpg\")):\n",
    "        #print(\"Processing file: {}\".format(ft2.rsplit('/')[-2]+\"/\"+ft2.rsplit('/')[-1]))\n",
    "        img = dlib.load_rgb_image(ft2)\n",
    "        #print(img.shape,type(img))\n",
    "        #------------------\n",
    "        temp_array_data.append(img)\n",
    "        temp_array_label.append(ft1.rpartition('/')[-1])\n",
    "        #------------------\n",
    "        #print(ft1.rpartition('/')[-1],type(ft1.rpartition('/')[-1]))\n",
    "        #print(type(temp_array_data[0]),temp_array_label[0])\n",
    "        \n",
    "        win.clear_overlay()\n",
    "        win.set_image(img)\n",
    "        dets = detector(img, 1)\n",
    "        #print(\"Number of faces detected: {}\".format(len(dets)))\n",
    "        \n",
    "        for k,d in enumerate(dets):\n",
    "            print(\"Detection {}: Left: {} Top: {} Right: {} Bottom: {}\".format(\n",
    "            k+1, d.left(), d.top(), d.right(), d.bottom()))\n",
    "            # Get the landmarks/parts for the face in box d.\n",
    "            shape = sp(img, d)\n",
    "            # Draw the face landmarks on the screen so we can see what face is currently being processed.\n",
    "            win.clear_overlay()\n",
    "            win.add_overlay(d)\n",
    "            win.add_overlay(shape)\n",
    "            #win.add_overlay()\n",
    "            #input(\"Press Enter to continue...\")\n",
    "            face_chip = dlib.get_face_chip(img, shape)        \n",
    "            #cv2.imwrite(\"aligned_face_n_{}.jpg\".format(k), face_chip)\n",
    "            #win.set_image(face_chip)\n",
    "            #win.clear_overlay()\n",
    "            #input(\"Press Enter to continue...\")\n",
    "            face_descriptor_from_prealigned_image = facerec.compute_face_descriptor(face_chip)\n",
    "            temp_array_embeddings.append(face_descriptor_from_prealigned_image)\n",
    "            cv2.waitKey(0)\n",
    "        \n",
    "    final_array_data.extend(temp_array_data)\n",
    "    final_array_label.extend(temp_array_label)\n",
    "    final_array_embeddings.extend(temp_array_embeddings)\n",
    "    \n",
    "final_array_label_np = np.array(final_array_label)\n",
    "final_array_embeddings_np =np.array(final_array_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlib\n",
    "import numpy as np\n",
    "\n",
    "class photo_embeddings:\n",
    "    ## this class is programmed the path_to_photos variable contains another carpets\n",
    "    ## where the carpets names are the labels of the images inside them\n",
    "    #\n",
    "    #no class attribute by now\n",
    "    #\n",
    "    def __init__(self,path_to_photos,predictor_path,face_rec_model_path):\n",
    "        self.path_to_photos=path_to_photos\n",
    "        self.predictor_path=predictor_path\n",
    "        self.face_rec_model_path=face_rec_model_path\n",
    "        self.detector = dlib.get_frontal_face_detector()\n",
    "        self.sp = dlib.shape_predictor(self.predictor_path)\n",
    "        self.facerec = dlib.face_recognition_model_v1(self.face_rec_model_path)\n",
    "        \n",
    "    def get_embeddings(self,):\n",
    "        \n",
    "        win = dlib.image_window()\n",
    "        final_array_embeddings=[]\n",
    "        final_array_label=[]\n",
    "\n",
    "        for ft1 in glob.glob(os.path.join(self.path_to_photos, \"*\")):\n",
    "\n",
    "            temp_array_label=[]\n",
    "            temp_array_embeddings=[]\n",
    "            \n",
    "            for ft2 in glob.glob(os.path.join(ft1,\"*.jpg\")):\n",
    "                \n",
    "                img = dlib.load_rgb_image(ft2)\n",
    "                \n",
    "                dets = self.detector(img, 1)\n",
    "                win.clear_overlay()\n",
    "                win.set_image(img)\n",
    "                \n",
    "                for k,d in enumerate(dets):\n",
    "                    print(str(ft2)+','+str(k)+'face')\n",
    "                    shape = self.sp(img, d)\n",
    "                    win.clear_overlay()\n",
    "                    win.add_overlay(d)\n",
    "                    win.add_overlay(shape)\n",
    "\n",
    "                    face_chip = dlib.get_face_chip(img, shape)        \n",
    "\n",
    "                    face_descriptor_from_prealigned_image = self.facerec.compute_face_descriptor(face_chip)\n",
    "                    temp_array_embeddings.append(face_descriptor_from_prealigned_image)\n",
    "                    temp_array_label.append(ft1.rpartition('/')[-1])\n",
    "\n",
    "            final_array_label.extend(temp_array_label)\n",
    "            final_array_embeddings.extend(temp_array_embeddings)\n",
    "        \n",
    "        final_array_label_np = np.array(final_array_label)\n",
    "        final_array_embeddings_np =np.array(final_array_embeddings)\n",
    "        \n",
    "        return final_array_embeddings_np,final_array_label_np\n",
    "    \n",
    "    def get_embedding(self,path_to_photo):\n",
    "        #\n",
    "        #return the embedding of just a one photo\n",
    "        #\n",
    "        #win = dlib.image_window()\n",
    "        \n",
    "        temp_array_label=[]\n",
    "        temp_array_embeddings=[]\n",
    "        img = dlib.load_rgb_image(path_to_photo)\n",
    "                \n",
    "        dets = self.detector(img, 1)\n",
    "        #win.clear_overlay()\n",
    "        #win.set_image(img)\n",
    "\n",
    "        for k,d in enumerate(dets):\n",
    "            shape = self.sp(img, d)\n",
    "            #win.clear_overlay()\n",
    "            #win.add_overlay(d)\n",
    "            #win.add_overlay(shape)\n",
    "\n",
    "            face_chip = dlib.get_face_chip(img, shape)        \n",
    "\n",
    "            face_descriptor_from_prealigned_image = self.facerec.compute_face_descriptor(face_chip)\n",
    "            temp_array_embeddings.append(face_descriptor_from_prealigned_image)\n",
    "            temp_array_label.append(path_to_photo.rpartition('/')[-1])\n",
    "        \n",
    "        #temp_array_label_np = np.array(temp_array_label)\n",
    "        #temp_array_embeddings_np =np.array(temp_array_embeddings)\n",
    "        \n",
    "        return np.array(temp_array_embeddings),np.array(temp_array_label)\n",
    "        \n",
    "    def get_prediction(self,object_model_trained,X_data):\n",
    "        #\n",
    "        #return the prediction of a vector of 128 D\n",
    "        #\n",
    "        y_pred = object_model_trained.predict(X_data)\n",
    "        self.y_pred=y_pred\n",
    "        return y_pred\n",
    "    \n",
    "    #def put_label(self,label):\n",
    "        #\n",
    "        #this method needs the y_pred result of the get_prediction method to work\n",
    "        #\n",
    "        #(a,b,c,d)=__rect_to_bb(rect)\n",
    "        #return y_label\n",
    "    \n",
    "    def __rect_to_bb(self,rect):\n",
    "        # take a bounding predicted by dlib and convert it\n",
    "        # to the format (x, y, w, h) as we would normally do\n",
    "        # with OpenCV\n",
    "        x = rect.left()\n",
    "        y = rect.top()\n",
    "        w = rect.right() - x\n",
    "        h = rect.bottom() - y\n",
    "\n",
    "        # return a tuple of (x, y, w, h)\n",
    "        return (x, y, w, h)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "###path_7 = os.path.join(\"images\",\"5_images\",\"val\")\n",
    "embeddings_obj_new = photo_embeddings(path_7,path_4,path_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images/5_images/val/jerry_seinfeld/httpafilesbiographycomimageuploadcfillcssrgbdprgfacehqwMTIwNjANjMMDINzIxNjcjpg.jpg,0face\n",
      "images/5_images/val/jerry_seinfeld/httpcdnssninsidercomwpcontentuploadsjerryseinfeldxjpg.jpg,0face\n",
      "images/5_images/val/jerry_seinfeld/httpaurorasblogcomwpcontentuploadsjerryseinfeldpublicityshotjpg.jpg,0face\n",
      "images/5_images/val/jerry_seinfeld/httpcdncdnjustjaredcomwpcontentuploadsheadlinesjerryseinfeldmakesbrianwilliamsjokejpg.jpg,0face\n",
      "images/5_images/val/jerry_seinfeld/httpblognjcomentertainmentimpactcelebritiesmediumjerrybjpg.jpg,0face\n",
      "images/5_images/val/madonna/httpcdncdnjustjaredcomwpcontentuploadsheadlinesmadonnatalksparisattackstearsjpg.jpg,0face\n",
      "images/5_images/val/madonna/httpecximagesamazoncomimagesIfmaBKWLACULSRjpg.jpg,0face\n",
      "images/5_images/val/madonna/httpassetsrollingstonecomassetsarticlemadonnadavidbowiechangedthecourseofmylifeforeversmallsquarexmadonnabowiejpg.jpg,0face\n",
      "images/5_images/val/madonna/httpassetsrollingstonecomassetsimagesalbumreviewaffaceabdcccaeedjpg.jpg,0face\n",
      "images/5_images/val/madonna/httpcdnfuncheapcomwpcontentuploadsVOGUEjpg.jpg,0face\n",
      "images/5_images/val/elton_john/httpcdncdnjustjaredcomwpcontentuploadsheadlineseltonjohnemmysperformancewatchnowjpg.jpg,0face\n",
      "images/5_images/val/elton_john/httpcdnlyricssongonlyricsnetwpcontentuploadsEltonJohnDiscographyCDreleasesjpg.jpg,0face\n",
      "images/5_images/val/elton_john/httpcdncdnjustjaredcomwpcontentuploadsheadlineseltonjohnsupportsbrucejennerstransitiontowomanjpg.jpg,0face\n",
      "images/5_images/val/elton_john/httpcdncdnjustjaredcomwpcontentuploadsheadlineseltonjohnstillstandingbrooklynnewyearsjpg.jpg,0face\n",
      "images/5_images/val/elton_john/httpafilesbiographycomimageuploadcfillcssrgbdprgfacehqwMTEODAOTcxNjcMjczMjkzjpg.jpg,0face\n",
      "images/5_images/val/ben_afflek/httpbpblogspotcomedLMjVpRGkSWexgsXjkNIAAAAAAAADWgFFtAUqBlhAsjpg.jpg,0face\n",
      "images/5_images/val/ben_afflek/httpwwwaceshowbizcomimagesphotobenaffleckjpg.jpg,0face\n",
      "images/5_images/val/ben_afflek/httpabsolumentgratuitfreefrimagesbenaffleckjpg.jpg,0face\n",
      "images/5_images/val/ben_afflek/httpafilesbiographycomimageuploadcfillcssrgbdprgfacehqwMTENDgMDUODczNDcNTcjpg.jpg,0face\n",
      "images/5_images/val/ben_afflek/httpcsvkmeuadecafjpg.jpg,0face\n",
      "images/5_images/val/mindy_kaling/httpcdnpastemagazinecomwwwarticlesmindykalingndbookjpg.jpg,0face\n",
      "images/5_images/val/mindy_kaling/httpcdncdnjustjaredcomwpcontentuploadsheadlinesmindykalingcomedypilotjpg.jpg,0face\n",
      "images/5_images/val/mindy_kaling/httpafilesbiographycomimageuploadcfillcssrgbdprgfacehqwMTIOTcwODQNTUzNjQMzcjpg.jpg,0face\n",
      "images/5_images/val/mindy_kaling/httpcdnpastemagazinecomwwwarticlesmindyprojectjpg.jpg,0face\n",
      "images/5_images/val/mindy_kaling/httpdbrbzkkbdsdcloudfrontnetwpcontentuploadsMindyKalingjpg.jpg,0face\n"
     ]
    }
   ],
   "source": [
    "X_data_new,y_labels_new = embeddings_obj_new.get_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovo', degree=3, gamma='auto', kernel='sigmoid',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_test_new = SVC(gamma='auto',kernel='sigmoid',decision_function_shape='ovo',probability=False)\n",
    "svm_test_new.fit(X_data_new,y_labels_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "                     weights='uniform')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.neighbors.nca import NeighborhoodComponentsAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "nca = NeighborhoodComponentsAnalysis(random_state=42)\n",
    "nca.fit(X_data_new, y_labels_new)\n",
    "nca.transform\n",
    "\n",
    "knn_model_test = KNeighborsClassifier()\n",
    "knn_model_test.fit(nca.transform(X_data_new),y_labels_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_9=os.path.join(\"images\",\"5_images\",\"val2\")\n",
    "#sklearn.metrics.accuracy_score(y_labels, y_pred)\n",
    "#y_pred\n",
    "def rect_to_bb(rect):\n",
    "    # take a bounding predicted by dlib and convert it\n",
    "    # to the format (x, y, w, h) as we would normally do\n",
    "    # with OpenCV\n",
    "    x = rect.left()\n",
    "    y = rect.top()\n",
    "    w = rect.right() - x\n",
    "    h = rect.bottom() - y\n",
    "\n",
    "    # return a tuple of (x, y, w, h)\n",
    "    return (x, y, w, h)\n",
    "\n",
    "\n",
    "predictor_path = path_4\n",
    "face_rec_model_path = path_3\n",
    "\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "sp = dlib.shape_predictor(predictor_path)\n",
    "facerec = dlib.face_recognition_model_v1(face_rec_model_path)\n",
    "\n",
    "#for ft1 in glob.glob(os.path.join(path_7, \"*\")):\n",
    "\n",
    "    #for ft2 in glob.glob(os.path.join(ft1,\"*.jpg\")):\n",
    "for ft2 in glob.glob(os.path.join(path_9,\"*.jpg\")):\n",
    "\n",
    "    #image=dlib.load_rgb_image(ft2)\n",
    "    image = cv2.imread(ft2)\n",
    "    image = imutils.resize(image, width=500)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    rects = detector(gray, 1)\n",
    "\n",
    "    #fname = ft1.rpartition('/')[-1]\n",
    "\n",
    "    # loop over the face detections\n",
    "    for (i, rect) in enumerate(rects):\n",
    "        #print(\"{} : faces\".format(i+1))\n",
    "        (x, y, w, h) = rect_to_bb(rect)\n",
    "        shape = sp(gray, rect)\n",
    "        #win.clear_overlay()\n",
    "        #win.add_overlay(rect)\n",
    "        #win.add_overlay(shape)\n",
    "        face_chip = dlib.get_face_chip(image, shape)        \n",
    "        face_descriptor_from_prealigned_image = facerec.compute_face_descriptor(face_chip)\n",
    "        #predicted_label=svm_test.predict(np.array(face_descriptor_from_prealigned_image).reshape(1,-1))\n",
    "\n",
    "        ### HERE, we are knowing the confidence rating for asigning a good label\n",
    "        nca_embeddings = nca.transform(np.array(face_descriptor_from_prealigned_image).reshape(1,-1))\n",
    "        confidence_rate = knn_model_test.predict_proba(nca_embeddings.reshape(1,-1))\n",
    "        confidence_rate= np.amax(confidence_rate[0])*100\n",
    "        #print(confidence_rate[0][0])\n",
    "        label='unknown'\n",
    "        if(confidence_rate>=65):\n",
    "            predicted_label=knn_model_test.predict(nca_embeddings.reshape(1,-1))\n",
    "            label=str(predicted_label[0])\n",
    "\n",
    "        clone = image.copy()\n",
    "        cv2.rectangle(clone, (x, y),(x + w, y + h), (0, 255, 0), 3)\n",
    "        startX = x\n",
    "        startY = y - 15 if y - 15 > 15 else y + 15\n",
    "        cv2.putText(clone, label + ' '+str(confidence_rate)+'%', (startX, startY),cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255), 2)\n",
    "        roi = image[y:y + h, x:x + w]\n",
    "        #cv2.imshow(\"Image\", clone)\n",
    "        #cv2.waitKey(0)\n",
    "        #cv2.destroyAllWindows()\n",
    "        cv2.imshow(\"Image\", clone)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "    \n",
    "    \n",
    "            \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_15=os.path.join(\"images\",\"5_images\",\"new_train\",\"pablo\",\"photoP1.jpg\")\n",
    "path_16=os.path.join(\"images\",\"local_images\",\"train_5\",\"percy\",\"PE5.jpg\")\n",
    "X_data_new,y_labels_new = embeddings_obj_new.get_embedding(path_16)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['mindy_kaling'], dtype='<U14')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_test_new.predict(X_data_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_16 = os.path.join(\"images\",\"local_images\",\"train_5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['elton_john'], dtype='<U14')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_model_test.predict(nca.transform(X_data_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING ON PHOTOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##train directory\n",
    "path_11=os.path.join(\"images\",\"local_images\",\"train_5\")\n",
    "##test_directory\n",
    "path_12=os.path.join(\"images\",\"local_images\",\"photos_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_obj_local = photo_embeddings(path_11,path_4,path_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images/local_images/train_5/pablo/photoP1.jpg,0face\n",
      "images/local_images/train_5/pablo/P7.jpg,0face\n",
      "images/local_images/train_5/pablo/photoP2.jpg,0face\n",
      "images/local_images/train_5/pablo/photoP5.jpg,0face\n",
      "images/local_images/train_5/donny/D9.jpg,0face\n",
      "images/local_images/train_5/donny/D1.jpg,0face\n",
      "images/local_images/train_5/donny/1918714_214177568292_679051_n.jpg,0face\n",
      "images/local_images/train_5/donny/D5.jpg,0face\n",
      "images/local_images/train_5/donny/D10.jpg,0face\n",
      "images/local_images/train_5/flor/F1.jpg,0face\n",
      "images/local_images/train_5/flor/F2.jpg,0face\n",
      "images/local_images/train_5/flor/f8.jpg,0face\n",
      "images/local_images/train_5/flor/F7.jpg,0face\n",
      "images/local_images/train_5/percy/photoPE2.jpg,0face\n",
      "images/local_images/train_5/percy/PE4.jpg,0face\n",
      "images/local_images/train_5/percy/PE5.jpg,0face\n"
     ]
    }
   ],
   "source": [
    "X_data_new,y_labels_new = embeddings_obj_local.get_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pablo' 'pablo' 'pablo' 'pablo' 'donny' 'donny' 'donny' 'donny' 'donny'\n",
      " 'flor' 'flor' 'flor' 'flor' 'percy' 'percy' 'percy']\n"
     ]
    }
   ],
   "source": [
    "X_data_new.shape\n",
    "print(y_labels_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=4, p=2,\n",
       "                     weights='uniform')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.neighbors.nca import NeighborhoodComponentsAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "nca = NeighborhoodComponentsAnalysis(random_state=42)\n",
    "nca.fit(X_data_new, y_labels_new)\n",
    "nca.transform\n",
    "\n",
    "knn_model_test = KNeighborsClassifier(n_neighbors=4)\n",
    "knn_model_test.fit(nca.transform(X_data_new),y_labels_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path_9=os.path.join(\"images\",\"5_images\",\"val2\")\n",
    "#sklearn.metrics.accuracy_score(y_labels, y_pred)\n",
    "#y_pred\n",
    "def rect_to_bb(rect):\n",
    "    # take a bounding predicted by dlib and convert it\n",
    "    # to the format (x, y, w, h) as we would normally do\n",
    "    # with OpenCV\n",
    "    x = rect.left()\n",
    "    y = rect.top()\n",
    "    w = rect.right() - x\n",
    "    h = rect.bottom() - y\n",
    "\n",
    "    # return a tuple of (x, y, w, h)\n",
    "    return (x, y, w, h)\n",
    "\n",
    "\n",
    "predictor_path = path_4\n",
    "face_rec_model_path = path_3\n",
    "\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "sp = dlib.shape_predictor(predictor_path)\n",
    "facerec = dlib.face_recognition_model_v1(face_rec_model_path)\n",
    "\n",
    "#for ft1 in glob.glob(os.path.join(path_7, \"*\")):\n",
    "\n",
    "    #for ft2 in glob.glob(os.path.join(ft1,\"*.jpg\")):\n",
    "for ft2 in glob.glob(os.path.join(path_12,\"*.jpg\")):\n",
    "\n",
    "    #image=dlib.load_rgb_image(ft2)\n",
    "    image = cv2.imread(ft2)\n",
    "    image = imutils.resize(image, width=500,height=500)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    rects = detector(gray, 1)\n",
    "\n",
    "    #fname = ft1.rpartition('/')[-1]\n",
    "\n",
    "    # loop over the face detections\n",
    "    for (i, rect) in enumerate(rects):\n",
    "        #print(\"{} : faces\".format(i+1))\n",
    "        (x, y, w, h) = rect_to_bb(rect)\n",
    "        shape = sp(gray, rect)\n",
    "        #win.clear_overlay()\n",
    "        #win.add_overlay(rect)\n",
    "        #win.add_overlay(shape)\n",
    "        face_chip = dlib.get_face_chip(image, shape)        \n",
    "        face_descriptor_from_prealigned_image = facerec.compute_face_descriptor(face_chip)\n",
    "        #predicted_label=svm_test.predict(np.array(face_descriptor_from_prealigned_image).reshape(1,-1))\n",
    "\n",
    "        ### HERE, we are knowing the confidence rating for asigning a good label\n",
    "        nca_embeddings = nca.transform(np.array(face_descriptor_from_prealigned_image).reshape(1,-1))\n",
    "        confidence_rate = knn_model_test.predict_proba(nca_embeddings.reshape(1,-1))\n",
    "        confidence_rate= np.amax(confidence_rate[0])*100\n",
    "        #print(confidence_rate[0][0])\n",
    "        label='unknown'\n",
    "        if(confidence_rate>=55):\n",
    "            predicted_label=knn_model_test.predict(nca_embeddings.reshape(1,-1))\n",
    "            label=str(predicted_label[0])\n",
    "\n",
    "        clone = image.copy()\n",
    "        cv2.rectangle(clone, (x, y),(x + w, y + h), (0, 255, 0), 3)\n",
    "        startX = x\n",
    "        startY = y - 15 if y - 15 > 15 else y + 15\n",
    "        cv2.putText(clone, label + ' '+str(confidence_rate)+'%', (startX, startY),cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255), 2)\n",
    "        roi = image[y:y + h, x:x + w]\n",
    "        #cv2.imshow(\"Image\", clone)\n",
    "        #cv2.waitKey(0)\n",
    "        #cv2.destroyAllWindows()\n",
    "        cv2.imshow(\"Image\", clone)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "    \n",
    "    \n",
    "            \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = numpy.linalg.norm(a-b)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
