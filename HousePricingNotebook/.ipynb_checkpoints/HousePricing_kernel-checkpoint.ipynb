{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing data\n",
    "df_train = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()\n",
    "print('------------------------------------------------------------')\n",
    "df_test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breve descripción numérica de los datos: \n",
    "En el train data tenemos 1460 datos con 81 columnas, mientras que el test data tenemos 1459 datos con 80 columnas (se entiende que es la data sobre la que tendremos que hacer nuestras predicciones)\n",
    "\n",
    "-Por eso, posteriormente a transformar los datos de train y test, tenemos que partir el train data en otros dos subsets y evaluarlo con eso. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "            Pasaremos a ver cuáles son las columnas que tienen valores nulos. Tanto en test como en train.\n",
    "            Aparte, meteremos la cantidad total de valores nulos dentro de un diccionario con su respectiva columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creo diccionarios\n",
    "null_dict_train = {}\n",
    "null_dict_test = {}\n",
    "for i in df_train.columns:\n",
    "    if df_train[i].isna().any() == True:\n",
    "        print('La columna', i, 'tiene', df_train[i].isna().sum(), 'valores nulos <--------')\n",
    "        null_dict_train[i] = df_train[i].isna().sum() #Creo los key and values de los diccionarios\n",
    "    else:\n",
    "        pass\n",
    "print('-------------------------------------------------------Ahora para test-------------------------------------------------')\n",
    "\n",
    "for i in df_test.columns:\n",
    "    if df_test[i].isna().any() == True:\n",
    "        print('La columna', i, 'tiene', df_test[i].isna().sum(), 'valores nulos <--------')\n",
    "        null_dict_test[i] = df_test[i].isna().sum()  #Creo los key and values de los diccionarios\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Según la teoría si el 30% de la data es nula se debería eliminar, qué columnas cumplen con esto?\n",
    "\n",
    "Teniendo en cuenta que nuesto df_train tiene 1460 valores y nuestro df_test tiene 1459 valores. Aplicaremos una función que nos saque el porcentaje para cada valor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Haremos dict comprehension\n",
    "#para cada key voy a sacarle el porcentaje de valores nulos con respecto a la data total\n",
    "null_dict_train_percentage = {k: (v/1460)*100 for k, v in null_dict_train.items()} \n",
    "null_dict_test_percentage = {k: (v/1459)*100 for k, v in null_dict_test.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "print(null_dict_train_percentage)\n",
    "print('-------------------------------------------------------------------------------------')\n",
    "print(null_dict_test_percentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### En una vista rápida diremos que:\n",
    " ##### Para el train dataframe:\n",
    "- Las columnas 'Alley', 'FireplaceQu', 'PoolQC', 'Fence' y 'MiscFeature' tienen más del 30% de la data nula\n",
    "\n",
    " ##### Para el test dataframe:\n",
    "- Las columnas 'Alley', 'FireplaceQu', 'PoolQC', 'Fence', 'MiscFeature' tienen más del 30% de la data nula.\n",
    "\n",
    "Entonces también concluimos que tanto los dataframe de test y train tienen la misma recurrencia en data nula por las mismas columnas.\n",
    "\n",
    "Ahora, hemos visto que las variables Alley, FireplaceQu, PoolQC y Fence no vendrìan a ser tan relevantes a la hora de costear una casa. Por lo tanto las eliminaremos. Notemos que la variable MiscFeature puede ser relevante. Por eso, reemplazaremos su valor nulo con 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_del = ['Alley','FireplaceQu','PoolQC','Fence']\n",
    "#delete this columns\n",
    "df_train.drop(cols_to_del, axis=1, inplace=True)\n",
    "df_test.drop(cols_to_del, axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filling nan with 0 values on MiscFeature column\n",
    "df_train['MiscFeature'].fillna(0, inplace= True)\n",
    "df_test['MiscFeature'].fillna(0, inplace = True)\n",
    "print(df_train['MiscFeature'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_corr = df_train.corr()\n",
    "plt.figure(figsize=(12,9))\n",
    "sns.heatmap(data_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que GarageArea y GarageCars tienen una correlación muy alta... Esto nos lleva alproblema de multicolinealidad. Entonces nos conviene eliminar una de estas columnas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se crea una lista con las columnas ha eliminar\n",
    "cols_to_del= ['GarageCars'] \n",
    "#se invoca la funcion drop, pasandole como parametro las columnas a eliminar tanto para df_train y df_test\n",
    "df_train.drop(cols_to_del, axis=1, inplace=True)\n",
    "df_test.drop(cols_to_del, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reemplazando valores nulos:\n",
    "\n",
    "Para comenzar con el criterio de reemplazo necesito una lista de las columnas tipo object y int/float: \n",
    "\n",
    "-Ya no necesito los números de valores nulos sino los tipos de data que hay dentro de la columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correremos denuevo la función de detección de nulos para obtener una lista de valores nulos global:\n",
    "null_dict_train_after = {}\n",
    "null_dict_test_after = {}\n",
    "#Es decir, almacenaremos los nulos en un diccionario\n",
    "for i in df_train.columns:\n",
    "    if df_train[i].isna().any() == True:\n",
    "        null_dict_train_after[i] = df_train[i].dtype #Creo los key and values de los diccionarios\n",
    "    else:\n",
    "        pass\n",
    "for i in df_test.columns:\n",
    "    if df_test[i].isna().any() == True:\n",
    "        null_dict_test_after[i] = df_test[i].dtype  #Creo los key and values de los diccionarios\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list_null_obj = []\n",
    "train_list_null_int = []\n",
    "\n",
    "for key,value in null_dict_train_after.items():\n",
    "    if value == 'object':\n",
    "        train_list_null_obj.append(key)\n",
    "    else:\n",
    "        train_list_null_int.append(key)\n",
    "\n",
    "test_list_null_obj = []\n",
    "test_list_null_int = []\n",
    "\n",
    "for key,value in null_dict_test_after.items():\n",
    "    if value == 'object':\n",
    "        test_list_null_obj.append(key)\n",
    "    else:\n",
    "        test_list_null_int.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_categorical(data, cols):\n",
    "    for i in cols:\n",
    "        sns.countplot(x = i, data = data[cols])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_categorical(df_train, train_list_null_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_categorical(df_test, test_list_null_obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez vistas las distribuciones de los valores podemos observar casos \"especiales\" en las variables BmstFinType1 y GarageFinish, donde el valor más recurrente no necesariamente difiere mucho del segundo más recurrente.\n",
    "\n",
    "Ahora, pasaremos a Reemplazar la data categórica:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_categorical(df, cols):\n",
    "    for i in cols:\n",
    "        df[i].fillna(value=df[i].value_counts().index[0],inplace =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_categorical(df_train, train_list_null_obj)\n",
    "replace_categorical(df_test, test_list_null_obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, reemplazaremos la data numérica. Pero antes veremos las distribuciones de las columnas nulas:\n",
    "\n",
    "-Se debe notar que: Si para reemplazar sumamos y restamos la media nos quedará un rango imposible de valores negativos para algunas variables.  Veamos: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_numerical(data, cols):\n",
    "    data_dropna = data.dropna()\n",
    "    for i in cols:\n",
    "        sns.distplot(data_dropna[i])\n",
    "        print('La media es:', data_dropna[i].mean(), 'La desv.stand es:', data_dropna[i].std())\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_numerical(df_train, train_list_null_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez vista la desviación estándar y la media podemos pensar dos veces en reemplazar randomizando los valores entre un rago [mean +- desv. standar] Por eso, notamos algunas cosas en cuanto estas variables:\n",
    "\n",
    "-MasVnrArea (en train y test): La resta de la desviación estándar nos puede dar un valor incoherente.\n",
    "\n",
    "-BsmtFinSF1 (en test): La resta de la desviación estándar nos puede dar un valor incoherente.\n",
    "\n",
    "-BsmtFinSF2 (en test): La resta de la desviación estándar nos puede dar un valor incoherente.\n",
    "\n",
    "-BsmtFullBath (en test): Solo tiene valores enteros (y es lógico eso)\n",
    "\n",
    "-BsmtHalfBath (en test): Solo tiene valores enteros (y es lógico eso)\n",
    "\n",
    "-GarageYearBuilt (en train y test): Solo tiene valores enteros (Y es lógico eso)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crearemos una función general para las variables nulas de train y test:\n",
    "def fill_nan_w_mean_std(df,col = 'col1'):\n",
    "    nan = df[df[col].isna()]\n",
    "    min_ = df[col].mean() - df[col].std()\n",
    "    max_ = df[col].mean() + df[col].std()\n",
    "    if min_ < 0: #Si el valor de reemplazo mínimo es 0, entonces redondearemos a uno\n",
    "        for i in nan.index:\n",
    "            random_num = random.randint(0,round(max_) + 1)\n",
    "            df[col].loc[i] = random_num\n",
    "    else: \n",
    "        for i in nan.index: #Ojo que los reemplazos son en sí redondeos.\n",
    "            random_num = random.randint(round(min_),round(max_) + 1)\n",
    "            df[col].loc[i] = random_num\n",
    "    df[col] = df[col].astype(int) #Se pone como integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for columnas in train_list_null_int:\n",
    "    fill_nan_w_mean_std(df_train, col = columnas)\n",
    "\n",
    "for columnas in test_list_null_int:\n",
    "    fill_nan_w_mean_std(df_test, col = columnas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se ha notado que hay un outlier con índice 1132. Entonces, se pasará a reemplazar ese valor. En GarageYrBlt el ouutlier es 2207 y debería decir 2007:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.loc[1132,'GarageYrBlt']= 2007"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.loc[666:667]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Crear una función que convierta las columnas de categorical a numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Función para remplazar los valores de una columna categorical a numerical.\n",
    "def asignar_variables(df,name_col): #Parametros: DataFrame,columna\n",
    "    dicc={}\n",
    "    col_with_val_unique = df[name_col].unique() # unique: filtra los valores unicos de una columna\n",
    "    contador=0\n",
    "    for i in col_with_val_unique:\n",
    "        dicc[i]=contador\n",
    "        contador+=1\n",
    "    df[name_col] = df[name_col].map(dicc) #map: agrega los values de un diccionario hacia la columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "dill.load_session('HousePricing1.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_cols = [*df_train.select_dtypes('object').columns] #Creamos la lista de columnas objeto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sacamos las dummies\n",
    "df_train = pd.get_dummies(df_train, columns = obj_cols)\n",
    "df_test = pd.get_dummies(df_test, columns = obj_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_cols_test = ['Utilities_NoSeWa','Condition2_RRAe','Condition2_RRAn','Condition2_RRNn','HouseStyle_2.5Fin',\n",
    "             'RoofMatl_ClyTile','RoofMatl_Membran','RoofMatl_Metal','RoofMatl_Roll','Exterior1st_ImStucc',\n",
    "             'Exterior1st_Stone','Exterior2nd_Other','Heating_Floor','Heating_OthW','Electrical_Mix','GarageQual_Ex','MiscFeature_TenC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hacemos esto porque hay columnas que tienen valores diferentes en el train como en el test.\n",
    "for new_col_test in null_cols_test:\n",
    "    df_test[new_col_test] = 0"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#No será necesario por ahora, que hemos creado dummies\n",
    "for col in obj_cols: \n",
    "    asignar_variables(df_train, col)\n",
    "    asignar_variables(df_test, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_test = df_test['Id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.drop('Id', axis = 1, inplace = True)\n",
    "df_test.drop('Id', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de comenzar a aplicar las predicciones, tenemos que separar la data. Porque tenemos que entrenar y testear por encima de todo el Train dataframe y el concurso nos pide predecir por sobre todo el Test dataframe. Entonces tendremos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dividimos la data en X e Y para poder entrenar el modelo\n",
    "from sklearn.model_selection import train_test_split\n",
    "Y = df_train['SalePrice']\n",
    "X = df_train.drop('SalePrice',axis=1, inplace = False)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aplicamos varios modelos predictivos: \n",
    "En este caso se usarán modelos de regresión:\n",
    "-Linear Regression\n",
    "\n",
    "-Polinomial Features?\n",
    "\n",
    "-Ridge o Lasso Regression?\n",
    "\n",
    "-Support Vector Regression\n",
    "\n",
    "-Decision Tree Regression\n",
    "\n",
    "-XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso, ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definimos la función que usaremos para el cross_validation: \n",
    "def cv_rmse():\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hacer PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = { 'max_depth' : [7,8,9,10] , 'max_features' : [11,12,13,14] ,\n",
    "               'max_leaf_nodes' : [None, 12,15,18,20] ,'min_samples_split' : [20,25,30],\n",
    "                'presort': [False,True] , 'random_state': [5] }\n",
    "nr_cv = 5    \n",
    "score_calc = 'neg_mean_squared_error'\n",
    "grid_dtree = GridSearchCV(DecisionTreeRegressor(), param_grid, cv=nr_cv, refit=True, verbose=1, scoring = score_calc)\n",
    "grid_dtree.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree_pred = grid_dtree.predict(df_test)\n",
    "sub_dtree = pd.DataFrame()\n",
    "sub_dtree['Id'] = id_test\n",
    "sub_dtree['SalePrice'] = dtree_pred\n",
    "sub_dtree.to_csv('dtreeregr.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "xgbr = XGBRegressor(n_estimators = 1000)\n",
    "\n",
    "xgbr.fit(X,Y,verbose = False)\n",
    "\n",
    "xgbr_pred = xgbr.predict(df_test)\n",
    "sub_xgbr = pd.DataFrame()\n",
    "sub_xgbr['Id'] = id_test\n",
    "sub_xgbr['SalePrice'] = xgbr_pred\n",
    "sub_xgbr.to_csv('xgbreg.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
